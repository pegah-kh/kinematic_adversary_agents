_target_: nuplan_gpu_work.planning.simulation.adversary_optimizer.king_optimizer.OptimizationKING
_convert_: 'all'


# https://www.youtube.com/watch?v=HeBZjSf1rl0 great video for LQR equation
# lecture on MPC https://web.stanford.edu/class/archive/ee/ee392m/ee392m.1056/Lecture14_MPC.pdf
# on bicycle model https://moorepants.github.io/eme134/lab-04.html

tracker:
  # _target_: nuplan_gpu_work.planning.simulation.adversary_optimizer.agent_tracker.agent_lqr_tracker.LQRTracker
  # _convert_: 'all'
  # LQR tuning
  q_longitudinal: [10.0]      # velocity tracking cost gain
  r_longitudinal: [1.0]       # acceleration tracking cost gain
  q_lateral: [1.0, 10.0, 0.0] # [lateral_error, heading_error, steering_angle] tracking cost gains [1.0, 10.0, 0.0]
  r_lateral: [1.0]            # steering_rate tracking cost gain
  # discretization_time: 0.1    # [s] The time interval used for discretizing the continuous time dynamics.
  tracking_horizon: 10      # The number of time steps (at discretization_time interval) ahead we consider for LQR.

  # Parameters for velocity and curvature estimation.
  jerk_penalty: 1e-4            # Penalty for jerk in velocity profile estimation.
  curvature_rate_penalty: 1e-2  # Penalty for curvature rate in curvature profile estimation.

  # Stopping logic
  stopping_proportional_gain: 0.5 # Proportional controller tuning for stopping controller
  stopping_velocity: 0.2          # [m/s] Velocity threshold for stopping

motion_model:
  _target_: nuplan_gpu_work.planning.simulation.motion_model.bicycle_model.BicycleModel
  _convert_: 'all'
  delta_t: 0.1


max_opt_iterations: 300
opt_iterations: 200

costs_to_use: ['drivable_us', 'fixed_dummy']
requires_grad_params : ['throttle', 'steer']
experiment_name: '0_0_01_1_0_20_50'
lrs: [0.,0.1]
loss_weights: [0.,20.]